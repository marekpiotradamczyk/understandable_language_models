{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcuda(tensor):\n",
    "    if CUDA:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_char(i, n):\n",
    "    x = torch.zeros(n)\n",
    "    x[i] = 1\n",
    "    return x.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.Wxh = Parameter(mcuda(torch.Tensor(in_dim, hidden_dim)))\n",
    "        self.Whh = Parameter(mcuda(torch.Tensor(hidden_dim, hidden_dim)))\n",
    "\n",
    "        self.bh = Parameter(mcuda(torch.Tensor(hidden_dim)))\n",
    "        self.activation = nn.Tanh()\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.Whh.size(1))\n",
    "        self.Wxh.data.uniform_(-stdv, stdv)\n",
    "        self.Whh.data.uniform_(-stdv, stdv)\n",
    "        self.bh.data.zero_()\n",
    "    \n",
    "    def forward(self, input, state):\n",
    "        preact = (\n",
    "            self.bh +\n",
    "            torch.matmul(input, self.Wxh) +\n",
    "            torch.matmul(state, self.Whh)\n",
    "        )\n",
    "        return self.activation(preact)\n",
    "    \n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.Why = Parameter(mcuda(torch.Tensor(self.hidden_size, self.vocab_size)))\n",
    "        self.by = Parameter(mcuda(torch.Tensor(self.vocab_size)))\n",
    "        self.reset_parameters()\n",
    "        self.init_rnns()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.Why.size(1))\n",
    "        self.Why.data.uniform_(-stdv, stdv)\n",
    "        self.by.data.zero_()\n",
    "\n",
    "    def init_rnns(self):\n",
    "        self.cell1 = RNNCell(self.vocab_size, self.hidden_size)\n",
    "        self.cell2 = RNNCell(self.hidden_size, self.hidden_size)\n",
    "        self.cell3 = RNNCell(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def init_states(self):\n",
    "        return [\n",
    "            mcuda(torch.zeros(self.hidden_size)),\n",
    "            mcuda(torch.zeros(self.hidden_size)),\n",
    "            mcuda(torch.zeros(self.hidden_size))\n",
    "        ]\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        for x in input:\n",
    "            x = mcuda(one_hot_char(x, self.vocab_size))\n",
    "            state1 = self.cell1(x, state[0])\n",
    "            state2 = self.cell2(state1, state[1])\n",
    "            state3 = self.cell3(state2, state[2])\n",
    "            state3 = self.dropout(state3)\n",
    "        \n",
    "        preds = nn.LogSoftmax(dim=1)(torch.matmul(state3, self.Why) + self.by)\n",
    "        \n",
    "\n",
    "        return preds, [state1, state2, state3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tinyshakespeare.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "corpus = data\n",
    "chars = set(corpus)\n",
    "vocab_size = len(chars)\n",
    "c2i = {c: i for i, c in enumerate(chars)}\n",
    "i2c = {i: c for i,c in enumerate(chars)}\n",
    "\n",
    "encoded_data = [c2i[char] for char in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more shows off\n",
      "Your wonder: but yet speak; first, you, my liege,\n",
      "Comes it not something near?\n",
      "\n",
      "LEONTES:\n",
      "Her natural posture!\n",
      "Chide me, dear stone, that I may say indeed\n",
      "Thou art Hermione; or rather, th\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, len(corpus) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return corpus[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_map_index(x):\n",
    "    return [c2i[char] for char in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_map_index(chunk[:-1])\n",
    "    target = char_map_index(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    hidden = model.init_states()\n",
    "    prime_input = char_map_index(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model([prime_input[p]], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model([inp], hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = i2c[top_i.item()]\n",
    "        predicted += predicted_char\n",
    "        inp = char_map_index(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    decoder.train()\n",
    "    hidden = decoder.init_states()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder([inp[c]], hidden)\n",
    "        loss += criterion(output, one_hot_char(target[c], vocab_size))\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), 5.0)\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6m 19s (4000 20%) 2.0249]\n",
      "Citizen: I dee sceath: I her to thou for be stonce\n",
      "so men she sirter the tou, cransow's browe youn's them se \n",
      "\n",
      "[12m 45s (8000 40%) 2.2972]\n",
      "Citizen:\n",
      "You do you afen\n",
      "bed bandess he asfore,\n",
      "Pome fayens iry but in will you, but,\n",
      "And siven for you disp \n",
      "\n",
      "[19m 13s (12000 60%) 1.9375]\n",
      "Citizen: but as the that cell your have bucking a vorise aud in all the rewer.\n",
      "\n",
      "LACHOLGS:\n",
      "Whit with ta thee  \n",
      "\n",
      "[25m 41s (16000 80%) 1.8572]\n",
      "Citizen:\n",
      "To mes us ernour have sto the crince to have ont ho to the been,\n",
      "Parst;\n",
      "In steasht seemmed in thy f \n",
      "\n",
      "[32m 6s (20000 100%) 2.1926]\n",
      "Citizen: the will head apeit is me even onstrelf\n",
      "To with which yad you are the snands to mant thy dar mayns  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20000\n",
    "print_every = 4000\n",
    "hidden_size = 150\n",
    "lr = 0.003\n",
    "\n",
    "decoder = CharRNN(vocab_size, hidden_size)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "#loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    #loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate(decoder,'Citizen:', 100), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You mard be casst thing hard in the man! is lett all in and po's, is far with he not mate this!\n",
      "Are a bond now all be me not misscord of of this man,\n",
      "I she oft my the sie! is the glaw your ke death and not mart: sir beens mad my to me seithous not the fewling mand toal do the brile so musent noble of three:\n",
      "\n",
      "QUENIO:\n",
      "Whom the pleat and 'ud comanf you moust,\n",
      "I' thoughts trind thou conning mape, whes thou us it.\n",
      "\n",
      "ILINIUSHe therefore now beain cioved y to the hones our love we'tisher and in the wirpe:\n",
      "The woule?\n",
      "\n",
      "MARENIUSHAR:\n",
      "Bot un of to is is donoted not blotelent me minger At nhear af the conges that I we here and tweo hy be him I man ould michmen: wrused of shwer me: I woo lord his not same the eve in some.\n",
      "Whee, wive:\n",
      "This it ut make not call enturs hould mar: sir more ind hang and cother mis with her nent not sting the greant off the censne im healing,\n",
      "Es that us sosther of so yhe him lord in I frosence wale who sen and onellous now ladd, comolt, and too yout day mand with un the creak.\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(decoder, corpus[:100], 1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
